{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "import csv\n",
    "import pprint\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = \"./traffic-signs-data/train.p\"\n",
    "testing_file = \"./traffic-signs-data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "x_train, y_train = train['features'], train['labels']\n",
    "x_test, y_test = test['features'], test['labels']\n",
    "\n",
    "# CSV file for sign names\n",
    "names_file = \"./signnames.csv\"\n",
    "\n",
    "# Load class names into dictionary\n",
    "with open(names_file, mode='r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    #next(reader)  # skip the header\n",
    "    signnames = {rows[0]: rows[1] for rows in reader}\n",
    "\n",
    "# Shuffle data used in model training\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "# Make sure the x and y data have same length\n",
    "assert (len(x_train) == len(y_train))\n",
    "assert (len(x_test) == len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Replace each question mark with the appropriate value.\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = len(x_train)\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = len(x_test)\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = x_train[0].shape\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "\n",
    "# Count the number of examples in each class and print them in detail with class description\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_info = {}\n",
    "for unique, counts in zip(unique, counts):\n",
    "    class_info[str(unique)] = {'description': signnames[str(unique)], 'count': counts}\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4, width=100)\n",
    "print(\"Example Count in Each Class\\n\")\n",
    "pp.pprint(class_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Data exploration visualization goes here.\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "#\n",
    "# Plot the first image\n",
    "#\n",
    "plt.figure(1)\n",
    "# plt.imshow(x_train[0].squeeze(), cmap='gray')\n",
    "plt.imshow(x_train[0])\n",
    "plt.title(class_info[str(y_train[0])]['description'])\n",
    "print('\\nCompleted plotting the first image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "grid = np.random.randint(n_train, size=(4, 4))\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8), subplot_kw={'xticks': [], 'yticks': []})\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.05)\n",
    "fig.suptitle('Random 16 images', fontsize=20)\n",
    "for ax, i in zip(axes.flat, list(grid.reshape(16, 1))):\n",
    "    # ax.imshow(x_train[int(i)].squeeze(), cmap='gray')\n",
    "    ax.imshow(x_train[int(i)])\n",
    "    title = str(i) + \" - \" + class_info[str(y_train[int(i)])]['description']\n",
    "    ax.set_title(title, fontsize=8)\n",
    "plt.show()\n",
    "#plt.savefig('./images/16_random_images')\n",
    "plt.close()\n",
    "print('\\nCompleted plotting the random 16 images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.figure(3)\n",
    "plt.bar(unique, counts, 0.5, color='b')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Each Class')\n",
    "plt.show()\n",
    "#plt.savefig('./images/class_freq_plot')\n",
    "print('\\nCompleted plotting class frequency bar plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grayscale(img):\n",
    "    \"\"\"\n",
    "    Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    you should call plt.imshow(gray, cmap='gray')\n",
    "    :param img: image to be converted to grayscale\n",
    "    :return: image in grayscale\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "def hist_equalize(img):\n",
    "    \"\"\"\n",
    "    Improve the contrast of image\n",
    "    Helps distribute the range of color in the image\n",
    "    Read more at\n",
    "    http://docs.opencv.org/trunk/d5/daf/tutorial_py_histogram_equalization.html\n",
    "    :param img:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return cv2.equalizeHist(img)\n",
    "def normalize_scale(img):\n",
    "    \"\"\"\n",
    "    Normalize images by subtracting mean and dividing by the range so that pixel values are between -0.5 and 0.5\n",
    "    :param img:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # normalized_image = np.divide(img - 125.0, 255.0)\n",
    "    normalized_image = (img - 125.0) / 255.0\n",
    "    return normalized_image\n",
    "def pre_processing(img_list):\n",
    "    \"\"\"\n",
    "    Call the grayscale, histogram equalization and normalization functions in the order and return images\n",
    "    with single channel\n",
    "    :param img_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    count = len(img_list)\n",
    "    shape = img_list[0].shape\n",
    "    processed = []\n",
    "\n",
    "    for i in range(count):\n",
    "        img = normalize_scale(hist_equalize(grayscale(img_list[i])))\n",
    "        processed.append(img)\n",
    "    \n",
    "    print(\"\\nPreprocessing of images complete..\\n\")\n",
    "    \n",
    "    return np.reshape(np.array(processed), [count, shape[0], shape[1], 1])\n",
    "def image_augmentation(img):\n",
    "\n",
    "    # References\n",
    "    # Geometric Transformations\n",
    "    # http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html\n",
    "    # Morphological Transformation\n",
    "    # http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html\n",
    "    # Smoothing Images\n",
    "    # http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html\n",
    "\n",
    "    # Perform smoothing or morphological transformation\n",
    "    # picker = np.random.randint(low=0, high=10)\n",
    "    picker = 1000\n",
    "\n",
    "    if picker == 1:\n",
    "        # Erosion\n",
    "        # Erodes away the boundaries of foreground objects - white region decreases in the image\n",
    "        erosion_kernel = np.ones((2, 2), np.uint8)\n",
    "        img_mod = cv2.erode(img, erosion_kernel, iterations=1)\n",
    "    elif picker == 2:\n",
    "        # Dilation\n",
    "        # Opposite of erosion\n",
    "        dilation_kernel = np.ones((2, 2), np.uint8)\n",
    "        img_mod = cv2.dilate(img, dilation_kernel, iterations=1)\n",
    "    elif picker == 3:\n",
    "        # Opening\n",
    "        # Erosion followed by dilation - removes noise\n",
    "        opening_kernel = np.ones((3, 3), np.uint8)\n",
    "        img_mod = cv2.morphologyEx(img, cv2.MORPH_OPEN, opening_kernel)\n",
    "    elif picker == 4:\n",
    "        # Closing\n",
    "        # Reverse of opening - dilation followed by erosion\n",
    "        closing_kernel = np.ones((3, 3), np.uint8)\n",
    "        img_mod = cv2.morphologyEx(img, cv2.MORPH_CLOSE, closing_kernel)\n",
    "    # elif picker == 5:\n",
    "    #     # Morphological Gradient\n",
    "    #     # Difference between the dilation and erosion of an image\n",
    "    #     gradient_kernel = np.ones((3, 3), np.uint8)\n",
    "    #     img_mod = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, gradient_kernel)\n",
    "    # elif picker == 6:\n",
    "    #     # Top hat\n",
    "    #     # Difference between the input image and the opening of the image\n",
    "    #     tophat_kernel = np.ones((7, 7), np.uint8)\n",
    "    #     img_mod = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, tophat_kernel)\n",
    "    elif picker == 7:\n",
    "        # Blur\n",
    "        img_mod = cv2.blur(img, (2, 2))\n",
    "    elif picker == 8:\n",
    "        # Gaussian blur\n",
    "        img_mod = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "    elif picker == 9:\n",
    "        # Median Blur\n",
    "        img_mod = cv2.medianBlur(img, 3)\n",
    "    else:\n",
    "        img_mod = img\n",
    "\n",
    "    # Rotation:\n",
    "    max_rotation_angle = 30.0  # degrees\n",
    "    max_center_translation = 5.0  # pixels\n",
    "    angle = np.random.uniform(low=-max_rotation_angle, high=max_rotation_angle)\n",
    "    center = tuple(np.array(img_mod.shape[0:2]) / 2.0)\n",
    "    center = (center[0] + np.random.uniform(low=-max_center_translation, high=max_center_translation),\n",
    "              center[1] + np.random.uniform(low=-max_center_translation, high=max_center_translation))\n",
    "\n",
    "    # Translation:\n",
    "    max_translation = 5.0  # pixels\n",
    "    x_translation = np.random.uniform(low=-max_translation, high=max_translation)\n",
    "    y_translation = np.random.uniform(low=-max_translation, high=max_translation)\n",
    "    translation_matrix = np.float32([[1, 0, x_translation],\n",
    "                                     [0, 1, y_translation]])\n",
    "    # Affine transformation:\n",
    "    # pts1 = np.float32([[5, 5],\n",
    "    #                    [20, 5],\n",
    "    #                    [5, 20]])\n",
    "    # pts2 = np.float32([[1, 10],\n",
    "    #                    [20, 5],\n",
    "    #                    [10, 25]])\n",
    "    # affine_transform_matrix = cv2.getAffineTransform(pts1, pts2)\n",
    "\n",
    "    # Perspective Transformation\n",
    "    # pts1 = np.float32([[3, 4], [29, 3], [4, 28], [27, 27]])\n",
    "    # pts2 = np.float32([[0, 0], [22, 0], [0, 22], [22, 22]])\n",
    "    # perspective_transform_matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    img_mod = cv2.warpAffine(img_mod, rot_mat, img_mod.shape[0:2], flags=cv2.INTER_LINEAR)\n",
    "    img_mod = cv2.warpAffine(img_mod, translation_matrix, img_mod.shape[0:2])\n",
    "    # result = cv2.warpAffine(img_mod, affine_transform_matrix, img_mod.shape[0:2])\n",
    "    # result = cv2.warpAffine(img_mod, perspective_transform_matrix, (22, 22))\n",
    "\n",
    "    # Brightness\n",
    "    brightness_multiplier = np.random.uniform(low=-0.25, high=0.25)\n",
    "    hsv = cv2.cvtColor(img_mod, cv2.COLOR_BGR2HSV)\n",
    "    hsv[:, :, 2] = hsv[:, :, 2] * (1.0 + brightness_multiplier)\n",
    "    result = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # plt.figure(1)\n",
    "    # plt.subplot(121)\n",
    "    # plt.imshow(img)\n",
    "    # plt.subplot(122)\n",
    "    # plt.imshow(result)\n",
    "    # plt.show()\n",
    "\n",
    "    return result\n",
    "def augment_training_images(x, y, params):\n",
    "    # Augment training set image set\n",
    "    if params['augment']:\n",
    "\n",
    "        target_class_size = params['augmented_class_size']\n",
    "\n",
    "        unq, unq_inv, unq_cnt = np.unique(y, return_inverse=True, return_counts=True)\n",
    "        class_index = np.split(np.argsort(unq_inv), np.cumsum(unq_cnt[:-1]))\n",
    "\n",
    "        new_training_x = []\n",
    "        new_training_y = []\n",
    "        for cls in unq:\n",
    "            if unq_cnt[cls] < target_class_size:\n",
    "                new_img_count = target_class_size - unq_cnt[cls]\n",
    "                for i in range(new_img_count):\n",
    "                    pck = np.random.choice(class_index[cls])\n",
    "                    new_training_x.append(image_augmentation(x[pck]))\n",
    "                    new_training_y.append(y[pck])\n",
    "        x = np.vstack((x, np.asarray(new_training_x)))\n",
    "        y = np.hstack((y, np.array(new_training_y)))\n",
    "\n",
    "    print(\"\\n Extra training images are generated... \\n\")\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre - processing parameters\n",
    "pre_process_param = {\n",
    "    'pre-process': True,\n",
    "    'mode': 1,\n",
    "    'augment': False,\n",
    "    'augmented_class_size': 3500\n",
    "}\n",
    "x_train, y_train = augment_training_images(x_train, y_train, pre_process_param)\n",
    "x_train = pre_processing(x_train)\n",
    "x_test = pre_processing(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Bar plot showing the count of each class in the training set after augmentation\n",
    "#\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.figure(3)\n",
    "plt.bar(unique, counts, 0.5, color='b')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Each Class after Augmentation')\n",
    "plt.show()\n",
    "#plt.savefig('./images/class_freq_plot_after_augment')\n",
    "print('\\nCompleted plotting class frequency bar plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Generate data additional data (OPTIONAL!)\n",
    "### and split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=832289)\n",
    "    \n",
    "n_train = len(x_train)\n",
    "n_valid = len(x_valid)\n",
    "n_test = len(x_test)\n",
    "    \n",
    "print(\"Training Set:   {} samples\".format(n_train))\n",
    "print(\"Validation Set: {} samples\".format(n_valid))\n",
    "print(\"Test Set:       {} samples\".format(n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement LeNet - 5 like architecture\n",
    "def lenet_model1(data_x, params, channel_count, keep_prob):\n",
    "\n",
    "    # The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels.\n",
    "\n",
    "    # Architecture\n",
    "    # Layer 1: Convolutional. The output shape is 28x28x32.\n",
    "    # Activation function\n",
    "    # Pooling. The output shape is 14x14x32.\n",
    "    # Layer 2: Convolutional. The output shape is 14x14x64.\n",
    "    # Activation function\n",
    "    # Layer 3: Convolutional. The output shape should be 10x10x128.\n",
    "    # Activation function\n",
    "    # Pooling. The output shape is 5x5x128.\n",
    "    # Flatten. Flatten the output shape of the final pooling layer such that it's 1D instead of 3D.\n",
    "    # The easiest way to do is by using tf.contrib.layers.flatten\n",
    "    # Layer 3: Fully Connected. This has 1064 outputs.\n",
    "    # Activation function\n",
    "    # Dropout\n",
    "    # Layer 4: Fully Connected. This has 532 outputs.\n",
    "    # Activation function\n",
    "    # Dropout\n",
    "    # Layer 5: Fully Connected. This has 256 outputs.\n",
    "    # Activation function\n",
    "    # Layer 6: Fully Connected (Logits). 43 class outputs.\n",
    "    # Output\n",
    "    # Return the result of the 2nd fully connected layer.\n",
    "\n",
    "    # Hyperparameters\n",
    "    mu = params['mean']\n",
    "    sigma = params['std']\n",
    "    chn = channel_count\n",
    "\n",
    "    layer_depth = {\n",
    "        'conv_1': 32,\n",
    "        'conv_2': 64,\n",
    "        'conv_3': 128,\n",
    "        'full_1': 1064,\n",
    "        'full_2': 532,\n",
    "        'full_3': 256,\n",
    "        'out': params['class_count']\n",
    "    }\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'conv_1': tf.Variable(tf.truncated_normal([5, 5, chn, layer_depth['conv_1']], \n",
    "                                                  mean=mu, stddev=sigma)),\n",
    "        'conv_2': tf.Variable(tf.truncated_normal([5, 5, layer_depth['conv_1'], layer_depth['conv_2']],\n",
    "                                                  mean=mu, stddev=sigma)),\n",
    "        'conv_3': tf.Variable(tf.truncated_normal([5, 5, layer_depth['conv_2'], layer_depth['conv_3']],\n",
    "                                                  mean=mu, stddev=sigma)),        \n",
    "        'full_1': tf.Variable(tf.truncated_normal([5 * 5 * layer_depth['conv_3'], layer_depth['full_1']], \n",
    "                                                  mean=mu, stddev=sigma)),\n",
    "        'full_2': tf.Variable(tf.truncated_normal([layer_depth['full_1'], layer_depth['full_2']],\n",
    "                                                  mean=mu, stddev=sigma)),\n",
    "        'full_3': tf.Variable(tf.truncated_normal([layer_depth['full_2'], layer_depth['full_3']],\n",
    "                                                  mean=mu, stddev=sigma)),\n",
    "        'out':    tf.Variable(tf.truncated_normal([layer_depth['full_3'], layer_depth['out']],\n",
    "                                                  mean=mu, stddev=sigma))\n",
    "    }\n",
    "    biases = {\n",
    "        'conv_1': tf.Variable(tf.zeros(layer_depth['conv_1'])),\n",
    "        'conv_2': tf.Variable(tf.zeros(layer_depth['conv_2'])),\n",
    "        'conv_3': tf.Variable(tf.zeros(layer_depth['conv_3'])),\n",
    "        'full_1': tf.Variable(tf.zeros(layer_depth['full_1'])),\n",
    "        'full_2': tf.Variable(tf.zeros(layer_depth['full_2'])),\n",
    "        'full_3': tf.Variable(tf.zeros(layer_depth['full_3'])),\n",
    "        'out':    tf.Variable(tf.zeros(layer_depth['out']))\n",
    "    }\n",
    "\n",
    "    # Layer 1: Convolutional. Input = 32x32xchn. Output = 28x28xlayer_depth['conv_1'].\n",
    "    conv1 = tf.nn.conv2d(data_x, weights['conv_1'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "    conv1 = tf.nn.bias_add(conv1, biases['conv_1'])\n",
    "\n",
    "    # Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # Pooling. Input = 28x28xlayer_depth['conv_1']. Output = 14x14xlayer_depth['conv_1'].\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    # conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "\n",
    "    # Layer 2: Convolutional. Input = 14x14xlayer_depth['conv_1']. Output = 14x14xlayer_depth['conv_2'].\n",
    "    conv2 = tf.nn.conv2d(conv1, weights['conv_2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.bias_add(conv2, biases['conv_2'])\n",
    "\n",
    "    # Activation.\n",
    "    conv2 = tf.nn.relu(conv2)    \n",
    "    \n",
    "    # Layer 2: Convolutional. Input = 14x14xlayer_depth['conv_2']. Output = 10x10xlayer_depth['conv_3'].\n",
    "    conv3 = tf.nn.conv2d(conv2, weights['conv_3'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "    conv3 = tf.nn.bias_add(conv3, biases['conv_3'])\n",
    "\n",
    "    # Activation.\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "\n",
    "    # Pooling. Input = 10x10xlayer_depth['conv_3']. Output = 5x5xlayer_depth['conv_3'].\n",
    "    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    # conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "\n",
    "    # Flatten. Input = 5x5xlayer_depth['conv_3']. Output = 1600.\n",
    "    fc1 = flatten(conv3)\n",
    "\n",
    "    # Layer 3: Fully Connected. Input = 1600. Output = layer_depth['full_1'].\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['full_1']), biases['full_1'])\n",
    "\n",
    "    # Activation.\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    # Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = layer_depth['full_1']. Output = layer_depth['full_2'].\n",
    "    fc2 = tf.add(tf.matmul(fc1, weights['full_2']), biases['full_2'])\n",
    "\n",
    "    # Activation.\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    # Dropout\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = layer_depth['full_2']. Output = layer_depth['full_3'].\n",
    "    fc3 = tf.add(tf.matmul(fc2, weights['full_3']), biases['full_3'])\n",
    "\n",
    "    # Activation.\n",
    "    fc3 = tf.nn.relu(fc3)\n",
    "    # fc3 = tf.nn.dropout(fc3, keep_prob)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = layer_depth['full_3']. Output = class_count.\n",
    "    logits = tf.add(tf.matmul(fc3, weights['out']), biases['out'])\n",
    "    \n",
    "    reg_term = params['l2_beta'] * (tf.nn.l2_loss(weights['conv_1']) + \n",
    "                                    tf.nn.l2_loss(weights['conv_2']) +\n",
    "                                    tf.nn.l2_loss(weights['conv_3']) +\n",
    "                                    tf.nn.l2_loss(weights['full_1']) + \n",
    "                                    tf.nn.l2_loss(weights['full_2']) +\n",
    "                                    tf.nn.l2_loss(weights['full_3']))\n",
    "\n",
    "    return logits, reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "\n",
    "model_param_list = {\n",
    "        'name': 'trial_6',\n",
    "        'epoch': 100,\n",
    "        'batch_size': 128,\n",
    "        'mean': 0.,\n",
    "        'std': 0.1,\n",
    "        'class_count': len(class_info),\n",
    "        'rate': 0.001,\n",
    "        'l2_beta': 0.001,\n",
    "        'dropout_prob': 0.5\n",
    "}\n",
    "\n",
    "model_name = model_param_list['name']\n",
    "channel_count = x_train[0].shape[2]\n",
    "\n",
    "# Placeholder for batch of input images\n",
    "model_x = tf.placeholder(tf.float32, (None, 32, 32, channel_count))\n",
    "# Placeholder for batch of output labels\n",
    "model_y = tf.placeholder(tf.int32, None)\n",
    "\n",
    "# One hot encode the training set - one vs all\n",
    "one_hot_y = tf.one_hot(model_y, model_param_list['class_count'])\n",
    "\n",
    "# Dropout only\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "result_logits, reg_adder = lenet_model1(model_x, model_param_list, channel_count, keep_prob)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(result_logits, one_hot_y)\n",
    "\n",
    "loss_operation = tf.reduce_mean(cross_entropy) + reg_adder\n",
    "    \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=model_param_list['rate'])\n",
    "\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "# Model evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(result_logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Save\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "# Run the training data through the pipeline to train the model\n",
    "# Before each epoch, shuffle the training set\n",
    "# After each epoch, measure the loss and accuracy on the validation set\n",
    "# Save the model after training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(x_train)\n",
    "    print(\"\\nTraining Model: {}\\n\".format(model_name))\n",
    "    for i in range(model_param_list['epoch']):\n",
    "        train_x, train_y = shuffle(x_train, y_train)\n",
    "        for offset in range(0, num_examples, model_param_list['batch_size']):\n",
    "            end = offset + model_param_list['batch_size']\n",
    "            batch_x, batch_y = x_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={model_x: batch_x, model_y: batch_y,\n",
    "                                                    keep_prob: model_param_list['dropout_prob']})\n",
    "\n",
    "        num_valid_examples = len(x_valid)\n",
    "        total_accuracy = 0.0\n",
    "        total_loss = 0.0\n",
    "        for offset2 in range(0, num_valid_examples, model_param_list['batch_size']):\n",
    "            batch_valid_x, batch_valid_y = x_valid[offset2:offset2 + model_param_list['batch_size']], \\\n",
    "                                           y_valid[offset2:offset2 + model_param_list['batch_size']]\n",
    "            accuracy, lss = sess.run([accuracy_operation, loss_operation],\n",
    "                                feed_dict={model_x: batch_valid_x, model_y: batch_valid_y, keep_prob: 1.0})\n",
    "            total_accuracy += (accuracy * len(batch_valid_x))\n",
    "            total_loss += (lss * len(batch_valid_x))\n",
    "        \n",
    "        validation_accuracy = total_accuracy / num_valid_examples\n",
    "        validation_loss = total_loss / num_valid_examples\n",
    "\n",
    "        print(\"EPOCH {}: Validation Accuracy = {:.3f}, Validation Loss = {:.3f}\".format(i + 1, \n",
    "                                                                                        validation_accuracy,\n",
    "                                                                                        validation_loss))\n",
    "\n",
    "    saver.save(sess, './models/' + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
